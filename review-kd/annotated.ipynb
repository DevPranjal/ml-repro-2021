{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Reproducibility Challenge 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        preact = out\n",
    "        out = F.relu(out)\n",
    "        if self.is_last:\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, depth, num_filters, block_name='BasicBlock', num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        # Model type specifies number of layers for CIFAR-10 model\n",
    "        if block_name.lower() == 'basicblock':\n",
    "            assert (\n",
    "                depth - 2) % 6 == 0, 'When use basicblock, depth should be 6n+2, e.g. 20, 32, 44, 56, 110, 1202'\n",
    "            n = (depth - 2) // 6\n",
    "            block = BasicBlock\n",
    "        elif block_name.lower() == 'bottleneck':\n",
    "            assert (\n",
    "                depth - 2) % 9 == 0, 'When use bottleneck, depth should be 9n+2, e.g. 20, 29, 47, 56, 110, 1199'\n",
    "            n = (depth - 2) // 9\n",
    "            block = Bottleneck\n",
    "        else:\n",
    "            raise ValueError('block_name shoule be Basicblock or Bottleneck')\n",
    "\n",
    "        self.inplanes = num_filters[0]\n",
    "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_filters[0])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, num_filters[1], n)\n",
    "        self.layer2 = self._make_layer(block, num_filters[2], n, stride=2)\n",
    "        self.layer3 = self._make_layer(block, num_filters[3], n, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(num_filters[3] * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.to('cuda')\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = list([])\n",
    "        layers.append(block(self.inplanes, planes, stride,\n",
    "                      downsample, is_last=(blocks == 1)))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes,\n",
    "                          is_last=(i == blocks-1)))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_feat_modules(self):\n",
    "        feat_m = nn.ModuleList([])\n",
    "        feat_m.append(self.conv1)\n",
    "        feat_m.append(self.bn1)\n",
    "        feat_m.append(self.relu)\n",
    "        feat_m.append(self.layer1)\n",
    "        feat_m.append(self.layer2)\n",
    "        feat_m.append(self.layer3)\n",
    "        return feat_m\n",
    "\n",
    "    def get_bn_before_relu(self):\n",
    "        if isinstance(self.layer1[0], Bottleneck):\n",
    "            bn1 = self.layer1[-1].bn3\n",
    "            bn2 = self.layer2[-1].bn3\n",
    "            bn3 = self.layer3[-1].bn3\n",
    "        elif isinstance(self.layer1[0], BasicBlock):\n",
    "            bn1 = self.layer1[-1].bn2\n",
    "            bn2 = self.layer2[-1].bn2\n",
    "            bn3 = self.layer3[-1].bn2\n",
    "        else:\n",
    "            raise NotImplementedError('ResNet unknown block error !!!')\n",
    "\n",
    "        return [bn1, bn2, bn3]\n",
    "\n",
    "    def forward(self, x, with_features=False, preact=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)  # 32x32\n",
    "        f0 = x\n",
    "\n",
    "        x, f1_pre = self.layer1(x)  # 32x32\n",
    "        f1 = x\n",
    "        x, f2_pre = self.layer2(x)  # 16x16\n",
    "        f2 = x\n",
    "        x, f3_pre = self.layer3(x)  # 8x8\n",
    "        f3 = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        f4 = x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if with_features:\n",
    "            if preact:\n",
    "                return [f0, f1_pre, f2_pre, f3_pre, f4], x\n",
    "            else:\n",
    "                return [f0, f1, f2, f3, f4], x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "def resnet44(num_classes=10):\n",
    "    return ResNet(44, [16, 16, 32, 64], 'basicblock', num_classes)\n",
    "\n",
    "def resnet56(num_classes=10):\n",
    "    return ResNet(56, [16, 16, 32, 64], 'basicblock', num_classes)\n",
    "\n",
    "def resnet110(num_classes=10):\n",
    "    return ResNet(110, [16, 16, 32, 64], 'basicblock', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet8(num_classes=10):\n",
    "    return ResNet(8, [16, 16, 32, 64], 'basicblock', num_classes)\n",
    "\n",
    "def resnet14(num_classes=10):\n",
    "    return ResNet(14, [16, 16, 32, 64], 'basicblock', num_classes)\n",
    "\n",
    "def resnet20(num_classes=10):\n",
    "    return ResNet(20, [16, 16, 32, 64], 'basicblock', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Based Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_and_match_channels(features2, features1, device):\n",
    "    \"\"\"Upsamples features2 to match features1\"\"\"\n",
    "    features2 = nn.Conv2d(\n",
    "        features2.shape[1],\n",
    "        features1.shape[1],\n",
    "        kernel_size=1\n",
    "    ).to(device)(features2)\n",
    "\n",
    "    return F.interpolate(features2, (features1.shape[2], features1.shape[3]))\n",
    "\n",
    "def abf(features1, features2, device):\n",
    "    features2 = upsample_and_match_channels(features2, features1, device)\n",
    "\n",
    "    features_concat = torch.cat((features1, features2), dim=1)\n",
    "    n, c, h, w = features_concat.shape\n",
    "\n",
    "    att_maps = nn.Conv2d(c, 2, kernel_size=1).to(device)(features_concat)\n",
    "    att_map1, att_map2 = att_maps[:, 0, :, :], att_maps[:, 1, :, :]\n",
    "    att_map1, att_map2 = att_map1.reshape(\n",
    "        (n, 1, h, w)), att_map2.reshape((n, 1, h, w))\n",
    "\n",
    "    return features1 * att_map1 + features2 * att_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial Context Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hcl(student_features, teacher_features):\n",
    "    total_loss = 0.0\n",
    "    n, c, h, w = student_features.shape\n",
    "\n",
    "    levels = [h, 4, 2, 1]\n",
    "    lvl_weight = 1.0\n",
    "    total_weight = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        example_loss = 0.0\n",
    "\n",
    "        for lvl in levels:\n",
    "            if lvl > h:\n",
    "                continue\n",
    "\n",
    "            lvl_sf = F.adaptive_avg_pool2d(student_features, (lvl, lvl))\n",
    "            lvl_tf = F.adaptive_avg_pool2d(teacher_features, (lvl, lvl))\n",
    "\n",
    "            lvl_loss = F.mse_loss(lvl_sf, lvl_tf) * lvl_weight\n",
    "            example_loss += lvl_loss\n",
    "\n",
    "            total_weight += lvl_weight\n",
    "            lvl_weight = lvl_weight / 2.0\n",
    "\n",
    "        total_loss += example_loss / total_weight\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(student, teacher, train_iter, loss, optimizer):\n",
    "    if is_pretrained_present(params, net_type):\n",
    "        print('using pretrained')\n",
    "        student = load_model(params, net_type)\n",
    "        return student\n",
    "\n",
    "    average_loss = RunningAverage()\n",
    "\n",
    "    print(\"\\nstarting training\")\n",
    "\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        with tqdm(total=len(train_iter)) as t:\n",
    "            for X, y in train_iter:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                student_features, student_preds = student(X, with_features=True)\n",
    "                with torch.no_grad():\n",
    "                    teacher_features, teacher_preds = teacher(\n",
    "                        X, with_features=True)\n",
    "\n",
    "                ce_loss = nn.CrossEntropyLoss()(student_preds, y)\n",
    "\n",
    "                student_features = student_features[::-1]\n",
    "                teacher_features = teacher_features[::-1]\n",
    "\n",
    "                total_kd_loss = 0\n",
    "\n",
    "                prev_abf_output = student_features[0]\n",
    "\n",
    "                for sf, tf in zip(student_features[1:], teacher_features[1:]):\n",
    "                    # print(\"SF:\\n\", sf.shape)\n",
    "                    # print(\"PREV ABF OUTPUT:\\n\", prev_abf_output.shape)\n",
    "                    abf_output = abf(sf, prev_abf_output, device)\n",
    "                    total_kd_loss += loss(abf_output, tf)\n",
    "                    prev_abf_output = abf_output\n",
    "                \n",
    "                total_loss = ce_loss + total_kd_loss * kd_loss_weight\n",
    "                # print(total_loss)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                average_loss.update(total_loss.item())\n",
    "                t.set_postfix(loss=f'{average_loss():.3f}')\n",
    "                t.update()\n",
    "\n",
    "    store_model(student, params, net_type)\n",
    "\n",
    "    return student"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3238a4b68a826aeff2550219200cebe12bf66a08bf22d5a4b99d8d44a2cfe5a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
